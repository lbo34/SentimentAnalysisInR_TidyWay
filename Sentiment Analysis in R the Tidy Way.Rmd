---
title: "Sentiment Analysis in R: The Tidy Way"
subtitle: "DataCamp course by Julia Silge"
author: "Laurent Barcelo"
date: "September, 22, 2018"
output: 
  html_notebook:
    toc: TRUE
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = F)
```

# 1st Segment - Tweets across the United States

## Sentiment lexicons

There are several different **sentiment lexicons** available for sentiment analysis. You will explore three in this course that are available in the `tidytext` package:

* `afinn` from Finn Årup Nielsen,
* `bing` from Bing Liu and collaborators, and
* `nrc` from Saif Mohammad and Peter Turney.

You will see how these lexicons can be used as you work through this course. The decision about which lexicon to use often depends on what question you are trying to answer.
In this exercise, you will use `dplyr`'s `count()` function. If you pass `count()` a variable, it will count the number of rows that share each distinct value of that variable.

#### Instructions
* Load the `dplyr` and `tidytext` packages.
* Add an argument to `get_sentiments()` to see what the `"bing"` lexicon looks like.
* Then call `get_sentiments()` for the `"nrc"` lexicon.
* Add an argument to `count()` so you can see how many words the `nrc` lexicon has for each sentiment category.

```{r}
library(dplyr)
library(tidytext)

get_sentiments("bing")
get_sentiments("nrc")

get_sentiments("nrc") %>% 
  group_by(sentiment) %>% 
  count()

get_sentiments("nrc") %>% count(sentiment)

```

## Implement an inner join

In this exercise you will implement sentiment analysis using an inner join. The `inner_join()` function from dplyr will identify which words are in both the sentiment lexicon and the text dataset you are examining. To learn more about joining data frames using dplyr, check out Joining Data in R with `dplyr`.

The `geocoded_tweets` dataset is taken from Quartz and contains three columns:

* `state`, a state in the United States
* `word`, a word used in tweets posted on Twitter
* `freq`, the average frequency of that word in that state (per billion words)

If you look at this dataset in the console, you will notice that the word "a" has a high frequency compared to the other words near the top of the sorted data frame; this makes sense! You can use `inner_join()` to implement a sentiment analysis on this dataset because it is in a tidy format.

#### Instructions
* In the console, take a look at the `geocoded_tweets` object.
* Use `get_sentiments()` to access the `"bing"` lexicon and assign it to bing.
* Use an `inner_join()` to implement sentiment analysis on the geocoded tweet data using the `bing` lexicon.

```{r eval = F}
load("/Users/lbarcelo/R_Repo/DataCamp/Sentiment Analysis in R: The Tidy Way/Data/geocoded_tweets.rda")

head(geocoded_tweets)
bing <- get_sentiments(lexicon = "bing")
tweets_bing <- geocoded_tweets %>% inner_join(bing)
```

## What are the most common sadness words?

After you have implemented sentiment analysis using `inner_join()`, you can use `dplyr` functions such as `group_by()` and `summarize()` to understand your results. For example, what are the most common words related to sadness in this Twitter dataset?

#### Instructions
Take a look at the `tweets_nrc` object, the output of an `inner join` just like the one you did in the last exercise. Then manipulate it to find the most common words that are related to sadness.

* Filter only the rows that have words associated with sadness.
* Group by word to find the average across the United States.
* Use the `summarize()` and `arrange()` verbs find the average frequency for each word, and then sort.
* Be aware that this is real data from Twitter and there is some use of profanity; the sentiment lexicons include profane and curse words.

```{r}
# data prep
nrc <- get_sentiments("nrc")
tweets_nrc <- geocoded_tweets %>% inner_join(nrc)

tweets_nrc %>% filter(sentiment == "sadness") %>% 
  group_by(word) %>% 
  summarise(freq = mean(freq)) %>% 
  arrange(desc(freq))
```

## What are the most common joy words?

You can use the same approach from the last exercise to find the most common words associated with joy in these tweets. Use the same pattern of `dplyr` verbs to find a new result.

#### Instructions
* First, filter to find only words associated with "joy".
* Next, group by word.
* Summarize for each word to find the average frequency freq across the whole United States.
* Arrange in descending order of frequency.

Now you can make a visualization using ggplot2 to see these results.
* Load the `ggplot2` package.
* Put words on the x-axis and frequency on the y-axis.
* Use `geom_col()` to make a bar chart. (If you are familiar with `geom_bar(stat = "identity")`, `geom_col()` does the same thing.)

```{r}
joy_words <- tweets_nrc %>% 
  filter(sentiment == "joy") %>% 
  group_by(word) %>% 
  summarise(freq = mean(freq)) %>% 
  arrange(desc(freq))

library(ggplot2)

joy_words %>%
  top_n(20) %>% 
  mutate(word = reorder(word, freq)) %>% #otherwise columns are not reordered in spite of the arrange() function
  ggplot(aes(x = word, y = freq)) +
  geom_col() +
  coord_flip()
```

## Do people in different states use different words?

So far you have looked at the United States as a whole, but you can use this dataset to examine differences in word use by state. In this exercise, you will examine two states and compare their use of joy words. Do they use the same words associated with joy? Do they use these words at the same rate?

#### Instructions
* Use the correct `dplyr` verb to find only the rows for the state of Utah.
* Add another condition inside the parentheses to find only the rows for the words associated with joy.
* Use the `dplyr` verb that arranges a data frame to sort in order of descending frequency.
* Repeat these steps for the state of Louisiana.

```{r}
tweets_nrc %>% 
  filter(state == "utah" & sentiment == "joy") %>%
  arrange(desc(freq))
 
tweets_nrc %>% 
  filter(state == "louisiana" & sentiment == "joy") %>%
  arrange(desc(freq)) 
```

## Which states have the most positive Twitter users?

For the last exercise in this chapter, you will determine how the overall sentiment of Twitter sentiment varies from state to state. You will use a dataset called `tweets_bing`, which is the output of an inner join created just the same way that you did earlier. Check out what `tweets_bing` looks like in the console.

You can use `group_by()` and `summarize()` to find which states had the highest frequency of positive and negative words, then pipe to `ggplot2` (after some `tidyr` manipulation) to make a clear, interesting visualization.

#### Instructions
* Choose variables in the call to `group_by()` so that you can `summarize()` by first state and the sentiment.
* After using `spread()` from `tidyr` and ungrouping, calculate the ratio of positive to negative words for each state.
* To make a plot, set up `aes()` so that states will go on the `x-axis` and the ratio will go on the `y-axis`.
* Add the correct `geom_*` layer to make points on the plot.
The call to `coord_flip()` flips the axes so you can read the names of the states more easily.

```{r}
library(tidyr)

tweets_bing %>% 
  group_by(state, sentiment) %>% 
  summarise(freq = mean(freq)) %>% 
  spread(sentiment, freq) %>% 
  ungroup() %>% 
  mutate(ratio = positive / negative, state = reorder(state, ratio)) %>% 
  ggplot(aes(x = state, y = ratio)) +
  geom_point() +
  coord_flip()
```


# 2nd Segment - Shakespeare gets Sentimental

## To be, or not to be

Let's take a look at the dataset you will use in this chapter to learn more about tidying text and sentiment analysis. The `shakespeare` dataset contains three columns:

* `title`, the title of a Shakespearean play,
* `type`, the type of play, either tragedy or comedy, and
* `text`, a line from that play.

This data frame contains the entire texts of six plays.

#### Instructions
* In the console, take a look at the `shakespeare` object.
* Pipe the data frame with the Shakespeare texts to the next line.
* Use `count()` with two arguments to find out which titles are in this dataset, whether they are tragedies or comedies, and how many lines they have.

```{r}
# data prep
load("/Users/lbarcelo/R_Repo/DataCamp/Sentiment Analysis in R: The Tidy Way/Data/shakespeare.rda")
head(shakespeare)
class(shakespeare)


shakespeare %>% count(title, type)

```

## Unnesting from text to word

The shakespeare dataset is not yet compatible with tidy tools. You need to first break the text into individual tokens (the process of tokenization); a token is a meaningful unit of text for analysis, in many cases, just synonymous with a single word. You also need to transform the text to a tidy data structure with one token per row. You can use tidytext’s `unnest_tokens()` function to accomplish all of this at once.

#### Instructions
* Load the `tidytext` package.
* Group by title to annotate the data frame by line number.
* Define a new column using `mutate()` called linenumber that keeps track of which line of the play text is from. * (Check out `row_number()` to do this!)
* Use `unnest_tokens()` to transform the non-tidy text data to a tidy text dataset.
* Pipe the tidy Shakespeare data frame to the next line.
* Use `count()` to find out how many times each word is used in Shakespeare's plays.

```{r}
library(tidytext)

tidy_shakespeare <- shakespeare %>% 
  group_by(title) %>% 
  mutate(linenumber = row_number()) %>% 
  unnest_tokens(word, text) %>% 
  ungroup()

tidy_shakespeare %>% 
  count(word, sort = T)
```

## Sentiment analysis of Shakespeare

You learned how to implement sentiment analysis with a join in the first chapter of this course. After transforming the text of these Shakespearean plays to a tidy text dataset in the last exercise, the resulting data frame tidy_shakespeare is ready for sentiment analysis with such an approach. Once you have performed the sentiment analysis, you can find out how many negative and positive words each play has with just one line of code.

#### Instructions
* Use the correct kind of join to implement sentiment analysis.
* Add the "bing" lexicon as the argument to the join function.
* Find how many positive and negative words each play has by using two arguments in `count()`.

```{r}
bing <- get_sentiments("bing")

shakespeare_sentiment <- tidy_shakespeare %>%
  inner_join(bing)

shakespeare_sentiment %>% count(title, sentiment)
```

## Tragedy or comedy?

The tidy dataset you created, `tidy_shakespeare`, is again available in your environment. Which plays have a higher percentage of negative words? Do the tragedies have more negative words than the comedies?

#### Instructions
First, calculate how many negative and positive words each play used.

* Implement sentiment analysis using the `"bing"` lexicon.
* Use `count()` to find the number of words for each combination of title, type, and sentiment.

Now, find the percentage of negative words for each play.

* Group by the titles of the plays.
* Find the total number of words in each play using `sum()`.
* Calculate a percent for each play that is the number of words of each sentiment divided by the total words in that play.
* Filter the results for only negative sentiment.

```{r}
sentiment_counts <- tidy_shakespeare %>% 
  inner_join(bing) %>% 
  count(title, type, sentiment)

sentiment_counts %>% 
  group_by(title) %>% 
  mutate(total = sum(n), percent = n/total*100) %>% 
  filter(sentiment == "negative") %>% 
  arrange(percent)
```

## Most common positive and negative words

You found in the previous exercise that Shakespeare's tragedies use proportionally more negative words than the comedies. Now you can explore which specific words are driving these sentiment scores. Which are the most common positive and negative words in these plays?

There are three steps in the code in this exercise. The first step counts how many times each word is used, the second step takes the top 10 most-used positive and negative words, and the final step makes a plot to visualize this result.

#### Instructions
* Implement sentiment analysis using the `"bing"` lexicon.
* Use `count()` to find word counts by sentiment.
* Group by sentiment so you can take the top 10 words in each sentiment.
* Notice what the line `mutate(word = reorder(word, n))` does; it converts word from a character that would be plotted in alphabetical order to a factor that will be plotted in order of n.

Now you can make a visualization of top_words using `ggplot2` to see these results.

* Put word on the x-axis and n on the y-axis.
* Use `geom_col()` to make a bar chart.

```{r}
word_counts <- tidy_shakespeare %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(sentiment, word) 

top_words <- word_counts %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% #if not ungrouped, the reorder would not work
  mutate(word = reorder(word, n))

ggplot(top_words, aes(x = word, y = n, fill = sentiment)) +
  geom_col(show.legend = F) +
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip()
```

## Word contributions by play

You have already explored how words contribute to sentiment scores for Shakespeare's plays as a whole. In this exercise, you will look at differences between titles. You will also practice using a different sentiment lexicon, the `"afinn"` lexicon in which words have a score from -5 to 5. Different lexicons take different approaches to quantifying the emotion/opinion content of words.

Which words contribute to the overall sentiment in which plays? In this exercise, you will look specifically at Macbeth.

#### Instructions
* Use `count()` to find how many times each word is used in each play.
* Implement sentiment analysis with the `"afinn"` lexicon. (Notice that it is possible to perform sentiment analysis on count data, not only the original tidy data frame.)
* Filter to only look at the sentiment scores for Macbeth; the title for Macbeth is "The Tragedy of Macbeth".
* In a second argument to `filter()`, only examine words with negative sentiment.

```{r}
tidy_shakespeare %>% 
  count(title, word, sort = T) %>% 
  inner_join(get_sentiments("afinn")) %>% 
  filter(title == "The Tragedy of Macbeth", score < 0 )
```

## Calculating a contribution score

In the last exercise, you saw how words in Macbeth were used different number of times and also had different sentiment scores in the "afinn" lexicon, from -5 to 5. Since this lexicon provides these scores for each word, you can calculate a relative contribution for each word in each play. This contribution can be found by multiplying the score for each word by the times it is used in each play and divided by the total words in each play.

#### Instructions
* Use `count()` to find how many times each word is used in each play.
* Implement sentiment analysis with the `"afinn"` lexicon.
* Group by the titles of the plays to get ready to calculate a total for each play in the next line.
* Calculate a contribution for each word in each play; the contribution can be found by multiplying each word's score by the times it is used in the play and divided by the total words in the play.

```{r}
sentiment_contributions <- tidy_shakespeare %>% 
  count(title, word, sort = T) %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(title) %>% 
  mutate(contribution = score * n / sum(n)) %>% 
  ungroup()

sentiment_contributions
```

## Alas, poor Yorick!

The `sentiment_contributions` that you calculated in the last exercise is available in your environment. It's time to explore some of your results! Look at Hamlet and The Merchant of Venice to see what negative and positive words are important in these two plays.

#### Instructions
* Look at `sentiment_contributions` in the console and use whatever strategy you like to find the exact titles for Hamlet and The Merchant of Venice. (Perhaps `count()`?)
* Filter for Hamlet and `arrange()` in ascending order (the default order) of contribution to see the words that contributed most negatively.
* Filter for The Merchant of Venice and `arrange()` in descending order of contribution to see the words that contributed most positively.

```{r}
sentiment_contributions %>%
  filter(title == "Hamlet, Prince of Denmark") %>% 
  arrange(contribution)

sentiment_contributions %>%
  filter(title == "The Merchant of Venice") %>% 
  arrange(desc(contribution))
```

## Sentiment changes through a play

In the last set of exercises in this chapter, you will examine how sentiment changes through the narrative arcs of these Shakespearean plays. We will start by first implementing sentiment analysis using `inner_join()`, and then use `count()` with four arguments:

* `title`,
* `type`,
* an `index` that will section together lines of the play, and
* `sentiment`.

After these lines of code, you will have the number of positive and negative words used in each `index`-ed section of the play. These sections will be 70 lines long in your analysis here. You want a chunk of text that is not too small (because then the sentiment changes will be very noisy) and not too big (because then you will not be able to see plot structure). In an analysis of this type you may need to experiment with what size chunks to make; sections of 70 lines works well for these plays.

#### Instructions
* Implement sentiment analysis using the `"bing"` lexicon.
* Use `count()` to find the number of words for each sentiment used in each play in sections, using four arguments.
* The first argument for `count()` maps to the plays themselves.
* The second argument keeps track of whether the play is a comedy or tragedy.
* The third argument is defined by you; call it index and set it equal to `linenumber %/% 70`. This index makes chunks of text that are 70 lines long using integer division (`%/%`).
* The fourth argument maps to the different sentiment categories.

```{r}
tidy_shakespeare %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(title, type, index = linenumber %/% 70, sentiment) ## VERY IMPORTANT TO REMEMBER THIS CODE  
```

## Calculating net sentiment

Now you will build on the code from the previous exercise and continue to move forward to see how sentiment changes through these Shakespearean plays. The next steps involve `spread()` from the tidyr package. After these lines of code, you will have the net sentiment in each index-ed section of the play; net sentiment is the negative sentiment subtracted from the positive sentiment.

#### Instructions
* Load the `tidyr` package.
* Use `spread()` to spread sentiment and n across multiple columns.
* Take a look at the output of the process after the `spread()` line in the console.
* Make a new column using `mutate()` that has the net sentiment found by subtracting negative sentiment from positive.

```{r}
library(tidyr)
tidy_shakespeare %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(title, type, index = linenumber %/% 70, sentiment) %>% 
  spread(key = sentiment, value = n, fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

## Visualizing narrative arcs

Now, in this final exercise exploring Shakespearean plays, you will continue to build on your manipulations of this text dataset and visualize the results of this sentiment analysis. How does sentiment change through the narrative arcs of these six plays by Shakespeare?

#### Instructions
* Load the ggplot2 package.
* Put `index` on the x-axis, `sentiment` on the y-axis, and use `type` for `fill`.
* Use `geom_col()` to make a bar chart. (If you are familiar with `geom_bar(stat = "identity")`, `geom_col()` does the same thing.)
* Call `facet_wrap()` to make a separate panel for each title; be sure to add `scales = "free_x"` so the x-axes behave nicely.

```{r}
library(ggplot2)

tidy_shakespeare %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(title, type, index = linenumber %/% 70, sentiment) %>% 
  spread(key = sentiment, value = n, fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  ggplot(aes(x = index, y = sentiment, fill = type)) +
  geom_col(show.legend = F) +
  facet_wrap(~ title, scales = "free_x")
```


# 3rd Segment - Analyzing TV news

## Tidying TV news

Take a look at the dataset of TV news text about climate change you will use in this chapter. The `climate_text` dataset contains almost 600 closed captioning snippets and four columns:

* `station`, the TV news station where the text is from,
* `show`, the show on that station where the text was spoken,
* `show_date`, the broadcast date of the spoken text, and
* `text`, the actual text spoken on TV.

Type `climate_text` in the console to take a look at the dataset before getting started with transforming it to a tidy format.

#### Instructions
* Load the `tidytext` package.
* Pipe the original dataset to the next line.
* Use `unnest_tokens()` to transform the non-tidy text data to a tidy text dataset, with a `word` column in the output.

```{r}
# data prep
load("/Users/lbarcelo/R_Repo/DataCamp/Sentiment Analysis in R: The Tidy Way/Data/climate_text.rda")

climate_text

library(tidytext)

tidy_tv <- climate_text %>% 
  unnest_tokens(input = text, output = word)
```

## Counting totals

Now that you have transformed the TV news data to a tidy data structure, you can find out what words are most common when discussing climate change on TV news, as well as the total number of words from each station. These are both helpful exploratory steps before moving on in analysis!

#### Instructions
* Find the most common words in this dataset with `count()` using `sort = TRUE`. (The command `anti_join(stop_words)` removes common words like "and", "of", and "to.")

You will now calculate the total number of words from each station, a quantity you'll use to find proportions later.

* Use `count()` with one argument to find how many words came from each stations.
* Change the name of the new column with `rename()` so that it is called `station_total` instead of `n`.

```{r}
tidy_tv %>% 
  anti_join(stop_words) %>% #removes stop words. dataset is included in the tidytext library
  count(word, sort = T)

tidy_tv %>% 
  count(station) %>% 
  rename(station_total = n)
```

## Sentiment analysis of TV news

After transforming the TV news texts to a tidy format in a previous exercise, the resulting data frame `tidy_tv` is ready for sentiment analysis using tidy data principles. Before you implement the inner join, add new column with the total number of words from each station so you can calculate proportions soon.

#### Instructions
* Define groups for each station in the dataset using `group_by()`.
* Make a new column called `station_total` in the dataframe that tallies the total number of words from each station; the `mutate()` verb will make a new column and the function `n()` counts the number of observations in the current group.
* Finally, implement sentiment analysis using the correct kind of join and the `"nrc"` lexicon as the argument to the join function.

```{r}
tv_sentiment <- tidy_tv %>% 
  group_by(station) %>% 
  mutate(station_total = n()) %>% 
  ungroup() %>% 
  inner_join(get_sentiments("nrc"))
```

## Which station uses the most positive or negative words?

You performed sentiment analysis on this dataset of TV news text, and the results are available in your environment in `tv_sentiment`. How do the words used when discussing climate change compare across stations? Which stations use more positive words? More negative words?

#### Instructions

Start off by looking at negative words.

* Define a new column `percent` using mutate() that is `n` divided by `station_total`, the proportion of words that belong to that sentiment.
* Filter only for the negative sentiment rows.
* Arrange by `percent` so you can see the results sorted by proportion of negative words.

Now repeat these steps to examine positive words!

```{r}
tv_sentiment %>% 
  count(station, sentiment, station_total) %>% 
  mutate(percent = n / station_total) %>%
  filter(sentiment == "negative") %>% 
  arrange(percent)

tv_sentiment %>% 
  count(station, sentiment, station_total) %>% 
  mutate(percent = n / station_total) %>%
  filter(sentiment == "positive") %>% 
  arrange(percent)
```

## Which words contribute to the sentiment scores?

It's important to understand which words specifically are driving sentiment scores, and when you use tidy data principles, it's not too difficult to check. In this exercise, you will make a plot showing which words contribute the most to the ten types of sentiment in the NRC lexicon. Look at the result, and think about which words might not be appropriate in these contexts. Are there proper names? Are there words which used in these contexts are neutral?

If so, you can always remove these words from your dataset (or the sentiment lexicon) using `anti_join()`.

#### Instructions
* Count by word and sentiment to find which words are contributing most overall to the sentiment scores.
* Group by sentiment.
* Take the top 10 words for each sentiment using `top_n()`.
* Set up the plot using `aes()`, with the words on the x-axis, the number of uses `n` on the y-axis, and fill corresponding to sentiment.

```{r}
tv_sentiment %>% 
  count(sentiment, word) %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(x = word, y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free") +
  coord_flip()
```

## Word choice and TV station

In the last exercise, you saw which words contributed to which sentiment for this dataset of closed captioning texts about climate change from TV news station. Now it's time to explore the different words that each station used in the context of discussing climate change. Which negative words did each station use when talking about climate change on the air?

## Instructions
* Filter for only negative words.
* Count by word and station to find which words are contributing most overall to the sentiment scores.
* Group by TV station.
* Take the top 10 words for each station.
* Set up the plot using `aes()`, with the words on the x-axis, the number of uses `n` on the y-axis, and `fill` corresponding to station.

```{r}
tv_sentiment %>% 
  filter(sentiment == "negative") %>% 
  count(word, station) %>% 
  group_by(station) %>% 
  top_n(10, wt = n) %>% 
  ungroup() %>% 
  mutate(word = reorder(paste(word, station, sep = "__"), n)) %>% ## without this it is not well ordered.
  ggplot(aes(x = word, y = n, fill = station)) +
  geom_col(show.legend = F) +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  facet_wrap(~station, nrow = 2, scales = "free") +
  coord_flip()
```

## Visualizing sentiment over time

You have compared how TV stations use positive and negative words; now it is time to see how sentiment is changing over time. Are TV news stations using more negative words as time passes? More positive words? You will use a function called `floor_date()` from the `lubridate` package to count uses of positive and negative words over time.

The `tidy_tv` dataframe you created near the beginning of this chapter is available in your environment.

#### Instructions
* Load the `lubridate` package.
* Define the new column with a `mutate()` statement using the `floor_date()` function, rounding each date down to the nearest 6-month unit.
* Group by the new `date` column (each 6 months).
* Implement sentiment analysis using the correct kind of join and the `"nrc"` sentiment lexicon.

Now you have a dataframe with the number of words per sentiment per 6 months, as well as the total words used in each 6 months!

* Filter for both positive and negative words so you can plot both.
* Count with three arguments: date, sentiment, and the total number of words.
* Set up your plot with `aes()` and put date on the x-axis, percent on the y-axis, and have color correspond to sentiment.

```{r}
library(lubridate)

sentiment_by_time <- tidy_tv %>% 
  mutate(date = floor_date(x = show_date, unit = "6 months")) %>% 
  group_by(date) %>% 
  mutate(total_words = n()) %>% 
  ungroup() %>% 
  inner_join(get_sentiments("nrc"))

sentiment_by_time %>% 
  filter(sentiment == "positive" | sentiment == "negative") %>% 
  count(date, sentiment, total_words) %>% 
  ungroup() %>% 
  mutate(percent = n / total_words) %>% 
  ggplot(aes(x = date, y = percent, col = sentiment)) +
  geom_line(size = 1.5) +
  geom_smooth(method = "lm", se = F, lty = 2) +
  expand_limits(y = 0)
```

## Word changes over time

You can also use tidy data principles to explore how individual words have been used over time. In the final exercise of this chapter, you will take the `tidy_tv` dataframe you created earlier and make a plot to see how certain words used in the context of climate change have changed in use over time. You will again use the `floor_date()` function, but this time to count monthly uses of words.

#### Instructions
* Define a new column within the `mutate()` statement with the `floor_date()` function, rounding each date down to the nearest 1-month unit.
* Count with 2 arguments: `date` and `word`.
* Set up your plot with `aes()` so that `date` is on the x-axis, `n` (the monthly number of uses) is on the y-axis), and color corresponds to `word`.
* Use `facet_wrap` to make a separate panel in your plot for each word.

```{r}
tidy_tv %>% 
  mutate(date = floor_date(show_date, unit = "1 month")) %>% 
  filter(word %in% c("threat", "hoax", "denier", "real", "warming", "hurricane")) %>% 
  count(date, word) %>% 
  ungroup() %>% 
  ggplot(aes(x = date, y = n, col = word)) +
  geom_line(size = 1.5, show.legend = FALSE) +
  facet_wrap( ~ word) +
  expand_limits(y = 0)
```

# 4th Segment - Singing a Happy Song (or Sad?!)

## Tidying song lyrics

Let's take a look at the dataset you will use in this final chapter to practice your sentiment analysis skills. The `song_lyrics` dataset contains five columns:

* `rank`, the rank a song achieved on the Billboard Year-End Hot 100,
* `song`, the song's title,
* `artist`, the artist who recorded the song,
* `year`, the year the song reached the given rank on the Billboard chart, and
* `lyrics`, the lyrics of the song.

This dataset contains over 5000 songs, from 1965 to the present. The lyrics are all in one column, so they are not yet in a tidy format, ready for analysis using tidy tools. It's your turn to tidy this text data!

#### Instructions
* Load the `tidytext` package.
* Pipe the `song_lyrics` object to the next line.
* Use `unnest_tokens()` to unnest the lyrics column into a new `word` column.

```{r}
# data prep
load("/Users/lbarcelo/R_Repo/DataCamp/Sentiment Analysis in R: The Tidy Way/Data/song_lyrics.rda")

library(tidytext)
tidy_lyrics <- song_lyrics %>% 
  unnest_tokens(input = lyrics, word)
```

## Calculating total words per song

For some next steps in this analysis, you need to know the total number of words sung in each song. Use `count()` to count up the words per song, and then `left_join()` these word totals to the tidy data set. You can specify exactly which column to use when joining the two data frames if you add `by = "song"`.

#### Instructions
* Count by song to find the word totals.
* With the `rename()` function, change the name of the new `n` column to `total_words`.
* Use `left_join()` to combine total with `tidy_lyrics` using the `song` column.

```{r}
totals <- tidy_lyrics %>%
  count(song) %>% 
  rename(total_words = n)

totals

lyric_counts <- tidy_lyrics %>% 
  left_join(totals, by = c("song" = "song"))
```

## Sentiment analysis on song lyrics

You have been practicing how to implement sentiment analysis with a join throughout this course. After transforming the text of these songs to a tidy text dataset and preparing the data frame, the resulting data frame `lyric_counts` is ready for you to perform sentiment analysis once again. Once you have done the sentiment analysis, you can learn which songs have the most sentiment words from the NRC lexicon. Remember that the NRC lexicon has 10 categories of sentiment:

* anger
* anticipation
* disgust
* fear
* joy
* negative
* positive
* sadness
* surprise
* trust

* Use the correct kind of join to implement sentiment analysis.
* Add the `"nrc"` lexicon as the argument to the join function.
* Find the songs with the most sentiment words by using two arguments in `count()`, along with `sort = TRUE`

```{r}
lyric_sentiment <- lyric_counts %>% 
  inner_join(get_sentiments("nrc")) 

lyric_sentiment %>% 
  count(song, sentiment, sort = T)
```

## The most positive and negative songs

You have successfully implemented sentiment analysis on this dataset of song lyrics, and now you can ask question such as, "Which songs have the highest proportion of positive words? Of negative words?" You calculated the total number of words for each song earlier, so now you need to count the number of words for each sentiment and song.

#### Instructions
* Use `count()` with three arguments to find the number of sentiment words for each song and total number of words.
* Make a new column using `mutate()` that is named percent, equal to `n` (the output of `count()`) divided by the `total number of words`.
* Filter for only negative words.
* Arrange by descending `percent`.

Now repeat these same steps for positive words.

```{r}
lyric_sentiment %>% 
  count(song, sentiment, total_words) %>% 
  ungroup() %>% 
  mutate(percent = n / total_words) %>% 
  filter(sentiment == "negative") %>% 
  arrange(desc(percent))

lyric_sentiment %>% 
  count(song, sentiment, total_words) %>% 
  ungroup() %>% 
  mutate(percent = n / total_words) %>% 
  filter(sentiment == "positive") %>% 
  arrange(desc(percent))
```

## Sentiment and Billboard rank

The `lyric_sentiment` data frame that you created earlier by using `inner_join()` is available in your environment. You can now explore how the sentiment score of a song is related to other aspects of that song. First, start with Billboard rank, how high on the annual Billboard chart the song reached. Do songs that use more positive or negative words achieve higher or lower ranks? Start with positive words, and make a visualization to see how these characteristics are related.

#### Instructions
* Count with three arguments: song, Billboard rank, and the total number of words that you calculated before
* Use the correct `dplyr` function to make two new columns, `percent` and a rounded version of `rank`
* Call the correct `ggplot2` `geom_*` to make a boxplot

```{r}
lyric_sentiment %>% 
  filter(sentiment == "positive") %>% 
  count(song, rank, total_words) %>% 
  ungroup() %>% 
  mutate(percent = n / total_words, rank = 10 * floor(rank / 10)) %>% 
  ggplot(aes(as.factor(rank), percent)) +
  geom_boxplot()
```

## More on Billboard rank and sentiment scores

In the last exercise, you explored how positive sentiment and Billboard rank are related using a visualization, and you found that there was no visible trend. Songs with more positive words do not reach higher or lower ranks on the Billboard chart. Next, check the same relationship using the same visualization but for negative words. The `lyric_sentiment` data frame that you created earlier is still available in your environment.

#### Instructions
* Filter for only negative words.
* Count using three arguments: `song`, Billboard rank, and total number of words.
* Define a new percent column with `mutate()` that is equal to `n` divided by `total_words`.

Then use `ggplot2` to make your visualization.

```{r}
lyric_sentiment %>% 
  filter(sentiment == "negative") %>% 
  count(song, rank, total_words) %>% 
  ungroup() %>% 
  mutate(percent = n / total_words, rank = 10 * floor(rank / 10)) %>%
  arrange(desc(percent)) %>% 
  ggplot(aes(x = as.factor(rank), y = percent)) +
  geom_boxplot()
  
```

## Sentiment scores by year

You are going to make two more exploratory plots in this exercise, much like the plots you made for Billboard rank. This time, you are going to explore how sentiment has been changing with time. Are songs on the Billboard chart changing in their use of negative or positive words through the decades?

#### Instructions
* Filter for only negative words.
* Use `count()` with three arguments to find the number of sentiment words for each song, year, and total number of words.
* Use `ggplot()` to set up a plot with year on the x-axis (remember to treat it as a factor with `as.factor(year)`) and percent on the y-axis.

Now repeat these same steps for positive words.

```{r}
lyric_sentiment %>% 
  filter(sentiment == "negative") %>% 
  count(song, year, total_words) %>% 
  ungroup() %>% 
  mutate(percent = n / total_words, year = 10 * floor(year / 10)) %>% 
  ggplot(aes(x = as.factor(year), y = percent)) +
  geom_boxplot()

lyric_sentiment %>% 
  filter(sentiment == "positive") %>% 
  count(song, year, total_words) %>% 
  ungroup() %>% 
  mutate(percent = n / total_words, year = 10 * floor(year / 10)) %>% 
  ggplot(aes(x = as.factor(year), y = percent)) +
  geom_boxplot()
```

## Modeling negative sentiment

You saw in your visualizations in the last exercise how positive and negative sentiment appear to be related to year. Now, you can explore that relationship with linear modeling. One more time, make a dataframe with one row per song that contains the proportion of negative words. Then, build a linear model and see whether the relationship is significant. The `lyric_sentiment` data frame that you created earlier is still available in your environment.

#### Instructions
* `Filter` for only negative words
* Use `mutate()` to define a new percent column that is `n` divided by `total_words`
* When fitting the linear model with `lm()`, `percent` will be your response and `year` will be your predictor.
* To see the results of your model fitting, call `summary` on your model fit object

```{r}
negative_by_year <- lyric_sentiment %>% 
  filter(sentiment == "negative") %>% 
  count(song, year, total_words) %>% 
  ungroup() %>% 
  mutate(percent = n / total_words)

model_negative <- lm(percent ~ year, data = negative_by_year)

summary(model_negative)
```

## Modeling positive sentiment

Now it's time for you to build a linear model for positive sentiment in this dataset of pop songs, just like you did for negative sentiment in the last exercise. Use the same approach and see what the results are!

#### Instructions
* `Count` using three arguments: `song`, `year`, and total number of words.
* Use `mutate()` to define a new `percent` column that is `n` divided by `total_words`
* Specify a linear model with `lm()` in the same way as the last exercise, but with `data = positive_by_year` this time.
* Explore the results of the model fitting with `summary()`

```{r}
devtools::install_github("abresler/gdeltr2")
```

